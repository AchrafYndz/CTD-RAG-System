Hello everyone, today we are going to be talking about the paper On the danger of sarcastic parrots, can language models be too big? This paper explores the risks that come with the rapid growth of large language models, or LLMs. So, we look at what those risks are, and what actions the authors recommend to help mitigate them. Before we dive into the risks though, it's important to understand how exactly we got here. So, NLP has changed dramatically over the past few decades. With each shift in the field, models have become more powerful, but also more resource intensive. It all started with engram models. These vary by counting sequences of words. And even back then, scaling helped. Some systems used trillions of engrams and showed steady improvement in tasks like translation. The next big step came with word embeddings like Word2Vec and GloVe. These letters represent words as vectors based on their context. This significantly reduced the amount of labelled data needed for downstream tasks. Afterwards came LSTM based models like ELMo. These modelled context dynamically and achieved state of the art performance, sometimes using just 1% of the data other models needed. The real revolution though came with transformers. Starting with models like BERT and eventually GPT-3 and beyond, transformers proved that larger models trained on more data tend to perform better across many tasks. GPT-3 for example has 175 billion parameters. Google's switch scene model goes even further with this with 1.6 trillion. So, these models don't just classify anymore. They can summarize, translate, answer questions and more, often with minimal fine-tuning needed. So, one of the clearest trends in NLP today is that models keep getting bigger. Not just in the number of parameters though, but also in the size of the datasets they're trained on. And while performance improves, we have to ask, how big is too big? And at what point do the costs outweigh the benefits? And that's where the risks come in. The paper highlights three major concerns. Environmental and financial costs, the problem with training data and the illusion of capability. At the heart of it is this. Progress in NLP has mostly been defined by scale. More data, more parameters and more computes, but without fully accounting for the consequences of this. So, let's dive into the environmental and financial costs of NLP. Training these large models consumes an enormous amount of energy. For example, one training run of a large transformer with neural architecture search emitted 284 tons of CO2. To give a perspective, this is about the same as yearly emissions of 57 every day. Even training a basic bird model uses as much energy as a transamerican flight. But, the costs go beyond just energy though. One estimate found that increasing a BLEU score by just 0.1 in machine translation, from English to German, required $150.000 worth of compute on top of the carbon emissions. While some of this energy comes from renewable sources, most cloud providers still rely heavily on non-renewable energy. But, even renewable sources have environmental costs, and the compute used for NLP takes up green energy that could as well be used elsewhere. And, what's more, the environmental impact isn't even distributed equally. The region is hit hardest by climate change. Most LLMs are trained in English and built to serve users in wealthy English-speaking countries. So, this raises a difficult question. Is it fair for marginalized communities to be trained in English? And, what's more, the environmental impact isn't even distributed equally. Most LLMs are trained in English and built to serve users in wealthy English-speaking countries. So, this raises a difficult question. Is it fair for marginalized communities to be trained in English? Is it fair for marginalized communities to bear the environmental burden of technology they rarely get to use? So, what exactly can we do about this? Well, the paper lays out a few actionable recommendations. Researchers should report energy usage and training time when they publish new models. This kind of transparency helps the community understand the real cost of progress. On top of that, it also helps to run models in carbon-friendly regions, places where the energy grid is cleaner, so emissions are therefore lower. But, beyond infrastructure, the focus of research needs to change as well. Instead of chasing small accuracy improvements at any costs, we should start valuing efficiency just as much. That means using more energy-efficient hardware and developing smarter algorithms as well, like model compression or distillation, that maintain performance while using fewer resources. Well, all of this ties into the idea of green AI. Building models that are not just powerful, but also environmentally responsible. There are tools available now to help researchers benchmark and reduce their energy usage, but the challenge is getting the field to adapt more widely. In this section, I will discuss how large datasets from the Internet encode biases. I will also discuss issues with dataset curation and the challenges of static data versus evolving social views. Models like GPT-3 are trained on datasets so vast, they are almost unimaginable. One petabyte is equivalent to 500 billion pages of text. These datasets are scrapped from the open web, like Wikipedia articles, news sites, and blogs. The issue with this dataset is not just the size, it's the lack of curation. These datasets are like dumping the entire Internet into a blender. Everything goes in, such as insightful essays, hate speech, medical misinformation. And all of this results in models running the ugliness of the web alongside its purity. Size versus diversity. Internet users overpresent young, male, Western, and affluent populations. For example, 67% of US Reddit users are men, while less than 15% of Wikipedia contributors are women. There is also a problem with the size of datasets. There is also a problem of filtering, which implies inclusion. For example, GPT-3 training data filters removed LGBTQ plus terms alongside hate speech. Content filters aim to remove toxicity, but they often overcorrect. Filters intentionally target hate speech, but sometimes they remove some words that are not meant to be removed. Static data versus evolving social views. Social movement and language changes. For example, Black Lives Matter shifted language around police violations, but static datasets lag behind. Also, language model trends on old data enforce outdated social views. For example, gendered job titles like waitress versus surfer. Updating large models is expensive, and fine-tuning requires thoughtful creation to capture evolving social perceptions. There is also another problem that the media doesn't cover everything. They often tend to ignore peaceful protests, amplifying violence, and state-arranged narratives. Encoding bias. LLM associates different words to gender. Example, a nurse being a female and a doctor being a male. There is also GPT-3 which generates toxic text even with neutral prompt. Also, BERT associates a disability term with negative sentiment. All of these are not just bugs, they are a direct reflection of the biased training data that we feed in these systems. There is also a problem of marginalising across multiple axes, like race, gender, and disability, where a woman will be less likely to be suggested for a STEAM role. Also, another problem is a tool used to detect bias which also misclassifies the actual word. For example, misclassifying cure as being toxic and also classifying macro-aggression as a perfect word. To improve the quality of datasets, it's better to actively include under-presented voices, also a document framework, and prioritise quality over scale. This section explores the risk of prioritising LLM-driven benchmarks over meaningful advancement. BERT, GPT-3, and similar models reversionalise NLP. They are trained on vast amounts of textual data using unsupervised learning and benchmarked on different datasets. They also achieve state-of-the-art results, leading to researchers focusing more on them. We can ask ourselves if LLM really understand the language. LLM manipulate linguistic forms but do not grasp the meaning. Their success relies on statistical correlation, not on comprehension. For example, if you change the first note to the second note with a double dot on O in sentiment analysis, models will fade. Also, different studies show that LLMs lack genuine semantic understanding. What are we losing by choosing the benchmark? By choosing the benchmark, we integrate a certain direction. There are some few resources on a certain direction, like grounded language models, and there is also little work on explainable semantics. There is also language inequality, where 90% of NLP's research focus more on English, and low-resource language lack even some basic tools. Many benchmarks designed to evaluate LLM contain patterns that the model runs to exploit without true understanding. For example, if you change the order of a sentence in a paragraph, many models struggle to understand the intended meaning. Despite their vast training data, LLM do not have deep structural understanding of the world. We should invest in smaller interpretable models and create linguistically diverse datasets and shift our focus to real-world applications. Exactly. And the next section is called stochastic parrots, which will soon become clear why. But first, we need to recognize that human language happens between two individuals who share some common ground. For example, take these two developers who are communicating with one another, and the one on the left can safely assume that the other developer understands terms like segfault and edgecase. However, if that same person has to explain the exact same ID to their boss, who is not a developer, they will likely use different terminology. So people adapt their language to the listener, and it's up to the listener to interpret what's being said. Text generated by an LLM is not grounded in communicative intent, and it doesn't know who it's talking to and does not adapt its answers based on who gave it the prompt. Instead, it's up to the reader to interpret the text. And this explains why, even though the LLM likely doesn't really understand what it's generating, its responses can still seem coherent to us. And this might feel counterintuitive, especially given how fluent LLM-generated text can be, but our perception of coherence comes from our own linguistic competence. And so that's why the authors like to refer to LLMs as stochastic parrots. The stochastic part coming from the fact that an LLM simply predicts the next token with some probability. And the parrot part comes from the fact that, like how parrots simply learn from what it hears people say, an LLM simply learns from what it has seen in its data, without any actual understanding of what it's generating. Now let's shift focus to the risks and potential harms associated with LLMs. The first risk the paper highlights comes from the over-representation of hegemonic views, which I would like to explain with a simple example. So suppose some bad country invades another country, killing a large portion of their population. Then, following such an attack, you will likely see lots of comments and posts on social media similar to these coming from the people in the aggressor country. However, the victims obviously won't be posting on the internet, meaning their side of the story doesn't get seen. And this is a common phenomenon, often summed up by the famous saying, history is written by the victors. So what does this issue have to do with LLMs? Well, since LLMs are often trained on all sorts of data from the internet, including posts from Reddit and other social media platforms, they can unintentionally replicate these biased views, amplifying the issue. And this amplification of a certain bias or view is not just limited to wars, but it also comes in many different forms. For example, since most of the training data is in English, and English speakers tend to have more Western norms, this leads to the over-representation of those norms and values, neglecting others from different cultures over the world. Similarly, biases around gender, race, history, and many other areas are also amplified. And this especially becomes an issue when LLMs are used for text classification tasks, such as resume screening. As an example, if the LLM learns from its data that good surgeons are typically male, then it might deem a female applicant as a bad fit for the job, simply based on her gender, even if she is actually highly qualified. Another risk caused by LLMs stems from the fact that LLMs can generate a lot more text than we as humans ever could before. So a fast typist might be able to type upwards of 140 words per minute. However, humans still need to think about what they're typing, and writing long texts takes energy and effort. LLMs, on the other hand, can produce massive amounts of text, and they don't get tired out, as you could in theory keep generating text 24-7. And this enables bad actors to generate malicious texts, such as spreading negative information about a country that they oppose. The next risk mentioned in the paper is related to translation. The authors argue that, because LLMs have gotten so good at generating coherent and fluent texts, it can give the illusion that the generated output must be accurate. As an example, if you were to translate the following sentence to French, and suppose you don't actually speak French, then just purely based on the fluency and grammatical correctness of this output, you would assume the translation is accurate, though that is not the case here. And so even with grammatically correct outputs, the translation can still be completely wrong. As an example, the authors refer to a case involving a Palestinian man who was arrested by Israeli police because of a mistranslation by Facebook's auto-translate feature. So the Palestinian had posted a picture to Facebook with the caption Good Morning in Arabic, but Facebook had translated the caption to Hurt Them in Hebrew, which led to his arrest. The final risk discussed in the paper is related to sensitive information. The authors highlight how sensitive information that is already available online, such as an address or phone number, can become more accessible by an LLM trained on that data. And so even though that information was already publicly available, it might have been very difficult to find, and the LLM has now made it much easier to access. The same can be said about non-personal sensitive information, such as tax evasion, and even though many LLMs have filters and rules set up so they don't help with these types of prompts, some of those safeguards can very easily be avoided by simply modifying the prompt to make it seem less malicious, though that is a different topic altogether. Now let's move on to the paths forward and explore the recommendations the authors make for future research on LLMs. The authors have some strategies to mitigate the mentioned risks. First, think before you build, so don't just dive in headfirst, but plan your datasets and models with purpose. Aim for fair benefits, meaning AI should help everyone and especially marginalized groups. Next, check the costs, as training models is expensive and can be harmful to the environment. Be efficient, this ties in with the last one, so we should optimize for energy and computation usage instead of just going for maximum raw power. Curate data wisely, meaning don't just scrape the whole internet, but instead select quality, unbiased data. And finally, more data does not mean less bias, as just training on more data does not magically fix fairness. And there is a quote in the paper that I think perfectly summarizes these last two points, which says that feeding AI systems on the world's beauty, ugliness, and cruelty, but expecting it to reflect only the beauty, is a fantasy. And this emphasizes that if we want to create a positive, fair, and unbiased model, we should attempt to train it on a positive, fair, and unbiased dataset. The authors also emphasize the importance of transparency in the data. Their recommendations include documenting your data, that is tracking data sources, collection methods, and motivations behind any choices. Next, the authors also recommend defining model uses, that means clearly stating what the model is suited for and what it's not suited for. Also, testing in all conditions, so performance should be benchmarked across different scenarios, and not just the one scenario where the model happens to excel. And finally, consider who is affected, that is both direct stakeholders and also people who are indirectly affected. Especially those at risk should be very carefully considered. The authors also strongly suggest reconsidering the goals of LLMs. They mention that the focus should not be put on creating the biggest model or getting the highest scores on benchmarks, but rather the focus should be on understanding the process, figuring out why a certain model performs so well on a specific set of tasks, and what this translates to in the real world. How can these models be applied, and how can they benefit society? And that concludes our presentation. Thank you for your attention.