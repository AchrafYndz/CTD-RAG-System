Toon Calders
​
Saartje Herman;
​
Xhejms Galanxhi;
​
Norea Sjunnesson
​
Dear students,

Please find attached the evaluation of your first presentation (the one in class).
Do not hesitate to contact me in case you have questions or remarks.

If you feel one of your team-mates made an exceptional contribution, let me know. 
Well-motivated nominations may result in an extra point for that team member.

Best regards,
Toon Calders

---

Group 5
Training language models to follow instructions with human feedback

Norea Sjunnesson - Xhejms Galanxhi - Saartje Herman

=================================================================================================

Positive points:
- overall a good presentation covering all important aspects of the paper
- effective use of examples to illustrate important concepts.

A few points of attention, however:
- the illustrations and examples stayed very close to those that were used in the paper.
- sometimes too many irrelevant details were given (e.g., the exact dropout rate, how many epochs used in training, etc.)
- very few details on the reinforcement learning. Given the target audience, a bit more of a gentle introduction into what is RL could have been helpful.

Comments on presentation style:
Saartje: overall well-presented, presented part and participation in the discussion was relatively limited. When presenting avoid too much just reading what is on the slide. 
Norea: well-structured, comprehensive, clear presentation 
Xhejms: overall well-presented, when presenting graphs make sure to clearly explain what can be read in the graph.

=================================================================================================
Comments from your fellow students:

Positive comments:
1. They discussed every step of the RLHF procedure in depth. I think the presentation was complete.
2. They explained everything nice and briefly
3. well given presentation
6. examples given making it easy to follow

Points for improvement:
2. Sometimes the slides where overloaded with text
6. lack of in depth knowledge of the paper

Scale ranges from Strongly disagree (1) to Strongly agree (6). Averages are given.
The presentation was clear and easy to follow: 4.14
The quality of the slides was high: 4.14
The students were clearly knowledgeable regarding the content of the paper: 4.29

---

Toon Calders
​
Ilias Sarikakis;
​
Xhejms Galanxhi;
​
Martina Janeva
​
Dear students,

Please find attached the preliminary evaluation of your recorded presentation. Please do not hesitate to contact me if anything is unclear. 
The peer review and the questions will follow on Saturday, once the deadline for sending the peer evaluations is over.

The evaluation is preliminary as you are allowed to update the presentation based on the comments. This is not compulsory; you can leave the presentation as-is and only submit the answer to the question you select. If you decide to update your presentation, please clearly indicate which changes you made in the comments of your submission.

21/4-25/4: your group will receive feedback from 3 other students and myself, and 3 questions. At this stage you may improve your presentation. For one of the 3 questions you receive, answer it in a separate video (Guideline: not more than 5 minutes is expected for the second video).

Best regards,
Toon Calders

---

Can LLMS keep a secret? Testing privacy implications of language models via contextual integrity theory
Ilias Sarikakis
Xhejms Galanxhi
Martina Janeva

Well-prepared and caefully designed presentation. All main points are covered without going into too many unneccessary details. The paper was relatively straightforward with not too many technical details, but nevertheless there were important nuances (eg difference with traditional privacy notions) that were properly captured.

Some points for improvement:
- although the presentation was overall very clear, it could benefit from adding in additional examples. The second speaker starts off very well by illustrating the different tiers with examples, but unfortunately for the later tiers, fewer examples are included. It would have been instructive, especially for tier 4, to see an example of a meeting that needed to be summarized (like there were examples in the appendix)
- when discussing the results, it would be helpful to include explicitly in the slide what measure is presented (correlation with human judgement, accuracy of predicting if an information is private or not, ...) Most of the times it is mentioned in the narrative, but it is easy to miss those mentions or they may come after the audience already started scrutinizing the tables.
- Some information that in my opinion is somewhat important, and that is missing (although there is a chance it was mentioned somewhere but I missed it): on how many scenario's were the experiments executed and where did the scenarios come from?
- It's outside of the scope of the paper, but nevertheless it would be interesting to know your thoughts on the validity of the experiments. For instance: when comparing with human judgement, what is the inter-annotator agreement? That is: are the humans always correct? For the labeled problems: where did they come from? Were they automatically generated? Human generated? Do they span a wide variety of domains or are they mainly restricted to the medical domain that was used in the examples?

Q: I would like to know how many instances each of the tiers contains, how they were generated, and for the human annotations, what were the inter-annotator agreement rates. What is your opinion about the validity of the experiments? Should they be imrpoved and if so, how?

---

Dear students, 

Please find attached the peer evaluations of your fellow students for the recorded presentation (assignment 2) and the questions. The order between the students' comments is random but consistent throughout the evaluation questions. My own question is the last one. As said before, you are free to select any of the questions.

Best regards,
Toon Calders

---

Group 5
Paper: Can LLMS keep a secret? Testing privacy implications of language models via contextual integrity theory


* My question to the presenters is the following:
1. The paper talks about CONFAIDE, which checks how well language models understand what should be kept private depending on the situation. What does this test show about where these models fail when it comes to protecting private information, and why should we care about that in real-life situations like using AI in emails or meetings?

2. How did human judgment transform into the evaluation of Tier 1 through Tier 3?

3. How was it ensured that tier 1 and 2 only outputed one of the possible options? And what with the results where it did not? <br>

4. I would like to know how many instances each of the tiers contains, how they were generated, and for the human annotations, what were the inter-annotator agreement rates. What is your opinion about the validity of the experiments? Should they be improved and if so, how?

* The presentation was clear and easy to follow: 4-4-5
* The quality of the slides was high: 5-4-4
* The students were clearly knowledgeable regarding the content of the paper: 5-5-4

* Positive points about the presentation:
1. - The presentation has clear and detailed examples (real-world scenarios) which help easy understanding.
   - Important concepts are highlighted in red, which helps to draw attention to key terms.    
   - Findings clearly show which models perform better and where they fail.

2. - The presentation slides and visuals were clear and engaging, which helped to maintain the audience's attention. 
   - The presentation covered the research paper very well within the given time, with good structure and flow.
   - The presentation breaks down the complex concepts of the paper into simple content with clear narration.

3. - The conclusion contained all importend aspects
   - The different tiers where clearly explained
   - The introduction explains clearly the goal of the paper and what is researched


* Points for improvement; constructive criticism:
1. - The presenters were speaking very fast, and it was hard to follow up. I think it is because of limited time.
   - Inconsistency in terminology: the slides switch in saying â€œsensitive infoâ€ and â€œprivate infoâ€ I think it is better to focus on one term.

2. - The first presenter was explaining a little fast, so sometimes I need to repeat a certain section in order to understand.
   - The voice of the second speaker was a little slower compared to other team members.
   - There were a few images in the results section that were a little hard to understand. In my opinion, easy example images should have been used for easy understanding. 

3. - the result slides are missing information like how large was the dataset to calculate the scores
   - Some actual examples of promps and answers of tier 3 and 4 would be usefull to understead how certain things where stated to the model like "how is the model told that not everyone knows all secrets"
   - The video volume could be adjusted so every person has the same audio level

---

Can LLMS keep a secret? Testing privacy implications of language models via contextual integrity theory
Ilias Sarikakis, Xhejms Galanxhi, Martina Janeva

Several concepts are not properly explained; some examples:
- What is a "Pew study"?
- What is Krippendorff's Alpha? It is likely beyond the scope of the presentation to explain the measure, but some information like is higher better, or lower better? And: what are typical cut-off values that are used to mark strong/weak agreement would have been very helpful.

For the different tiers, adding in some examples of instances and some examples of prompts and generated instances for those that were automatically generated, would have increased the understandability tremendously. In its current form the presentation remains quite abstract.

The presenter is mainly reading out the slides. It would have been helpful to add in some examples and visuals in the slides (eg some schematic of the generation process) and have the presenter explain the visuals.

Overall the question is answered correctly and completely, but not in the most optimal way. The answer is factual, but requires the audience to do additional lookups to completely understand the answer. This is because of on the one hand a lack of being self-contained (referring to external studies without specifying the main characteristics of the study) and on the other hand, the absence of concrete examples of prompts to generate instances and the instances themselves. 
