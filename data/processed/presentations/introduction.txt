[Slide 1]
Current Trends in Data Science and Artificial Intelligence Toon Calders Dept. Computer Science University of Antwerp toon.calders@uantwerpen.be

[Slide 2]
Course Topics ▪Large language models ▪Architecture of different large language models • GPT2, BERT, T5, GPT3 ▪Chain-of-thought (reasoning) ▪Reinforcement learning with human feedback ▪Issues with LLMs ▪Hallucination, privacy, bias and stereotypes ▪Using LLMs ▪Fine-tuning ▪Retrieval-augmented generation

[Slide 3]
3 Course Schedule 13-2-2025 Introduction: course organization ; deep learning; NLP 20-2-2025 Presentation: Attention is all you need 27-2-2025 Presentation: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Presentation: Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer 6-3-2025 Presentation: Language Models are Few-Shot Learners (GPT-3) Presentation: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models 13-3-2025 Presentation: Training language models to follow instructions with human feedback Presentation: Sparks of Artificial General Intelligence: Early experiments with GPT-4 20-3-2025 Paper 1 Paper 2 27-3-2025 Paper 3 Paper 4 3-4-2025 Paper 5 Paper 6 10-4-2025 paasvakantie 17-4-2025 paasvakantie 24-4-2025 preparation time / invited speaker 30-4-2025 preparation time / invited speaker 8-5-2025 preparation time / invited speaker 15-5-2025 Demo & poster session: application (RAG, fine-tuning, verify paper results) 22-5-2025 Demo & poster session: application (RAG, fine-tuning, verify paper results) ▪3 assignments ▪2 presentations ▪1 poster ▪Groups of 3 students ▪Based on your paper preference ▪Groups assigned by me ▪Groups change between assignments ▪Invited speakers

[Slide 4]
Course evaluation 4 ▪Presentations & demo/poster: ▪Graded by lecturer • Individual score ▪Feedback by fellow students • Key take-away • Positive points – points for improvement ▪Peer evaluation within group ▪Permanent evaluation ▪Small test at the end of the lecture

[Slide 5]
Block 1 papers 5 ▪Attention is all you need ▪BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding ▪Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer ▪Language Models are Few-Shot Learners (GPT-3) ▪Chain-of-Thought Prompting Elicits Reasoning in Large Language Models ▪Training language models to follow instructions with human feedback ▪Sparks of Artificial General Intelligence: Early experiments with GPT-4

[Slide 6]
Block 2 papers 6 ▪Measuring Fairness with Biased Rulers: A Comparative Study on Bias Metrics for Pre-trained Language Models ▪Distilling Step-by-Step! Outperforming Larger Language Models with Less Training Data and Smaller Model Sizes ▪On the Dangers of Stochastic Parrots: Can Language Models Be Too Big? ▪An empirical study of LLaMA3 quantization: from LLMs to MLLMs ▪Can LLMS keep a secret? Testing privacy implications of language models via contextual integrity theory ▪Red Teaming Language Models to Reduce Harms: Methods, Scaling Behaviors, and Lessons Learned ▪Red Teaming Language Models with Language Models ▪Bias and Fairness in Large Language Models: A Survey

[Slide 7]
Block 3 7 ▪Propose your own idea ▪Present a third paper ▪Try out some ideas that were presented ▪Build a RAG system ▪Finetune a model ▪Build a simple application based on LLMs ▪…
