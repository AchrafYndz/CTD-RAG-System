Hello everyone, welcome to our presentation in which Asima, Niels and myself, Jonas, will discuss the content of the paper Red Teaming Language Models with Language Models. As the title of the paper already suggests, this paper mainly focuses on the use of language models, or also often called LMs. Today, language models are incorporated and used in many different applications, including conversational assistants, question answering systems, etc. Because of this, it is crucial that these language models function and behave properly. However, it is observed that deployed language models may cause harm to users using them. Harm can be caused in many different forms, which we will discuss later on in this presentation, and is often caused in hard-to-predict ways. Therefore, being able to identify these harmful behaviors before deploying a language model is very important, mainly because we can use this information to try to correct the language model to prevent generating these harmful behaviors. Prior work on identifying harmful behaviors already existed before the introduction of this paper, but mainly focused on using human annotators to generate test cases. In other words, people have to manually create these test cases, making it expensive first of all, and this also limits the diversity and number of generated test cases. The quality of the test cases thus highly depends on human capabilities. To address this, this work proposes an automated approach to complement this manual testing. The high-level idea of the approach is that a language model is used to red-team the target language model, where red-teaming refers to automatically finding weak spots of the target language model that will cause it to behave harmfully. To evaluate whether the target language model generates harmful output, a separate classification model is used. Essentially, the goal of this approach is to automatically discover test cases that cause the target language model to be harmful. We can formally describe this approach by using the following components. First, we have the red LM denoted by PR, which is the model responsible for generating the test cases. The target LM, denoted by PT, is the model we want to test. The third component is the classifier denoted by R, which predicts if output Y is harmful given input X. Both inputs X and outputs Y represent natural language text. Using this information, we can define the approach according to three stages. In the first stage, we generate test cases using the red LM. Secondly, the generated test case is passed to the target LM that generates an output Y. Finally, in the last step, both test case X and output Y are passed to the red classifier, which then outputs the probability that this output Y is harmful given X. These three stages are repeated over and over again to find test cases that lead to harmful output. In the first stage, we generate test cases, but we didn't exactly specify how these are generated. In this paper, we refer to this generation process by test case generation methods. The goal of these methods is to learn how to produce a distribution over the inputs such that we will find inputs that often result in harmful outputs. Each method has its trade-off between diversity on the one hand, which indicates how diverse the generated test cases are, and difficulty on the other hand, which indicates the likelihood that it will elicit harmful output. In this paper, the red LM-PR is a large pre-trained language model to ensure that the generated test cases are well-formed natural language. This paper contains four different test case generation methods. The first method is called zero-shot generation. The idea behind this method is to sample many generations of test cases from a pre-trained language model using a prompt. This prompt influences the distribution of generated test cases, which helps us guide testing for particular behaviors. In general, designing effective prompts is non-trivial, but the authors of this paper found that simple one-sentence prompts are actually effective. Also, they were able to find such effective prompts in a relatively short amount of time. Something important to notice is that this fine-tuning of prompts requires human evaluation, meaning that zero-shot generation is not completely automated. The next method is called stochastic few-shot generation. The idea of this method is that we will treat failing zero-shot test cases, which were generated by for instance the previously discussed method, as examples for few-shot learning. In other words, we have the zero-shot prompt, to which we append several few-shot examples, which are basically the failing zero-shot test cases. This new prompt is then used to sample from the RETLM. For this method, we can increase the test case diversity by randomly subsampling a fixed number of test cases from a pool of test cases and adding these to the prompt used by the RETLM. To increase the difficulty of the generated test cases, we increase the likelihood of sampling a test case that leads to a harmful output. The third method is based on supervised learning. The idea is that we fine-tune the pre-trained language model, which is the RETLM in our setting, to maximize the log-likelihood of failing zero-shot test cases. So in other words, the model is fine-tuned to make failing zero-shot test cases more likely to be generated by the RETLM. The dataset for supervised learning is split using random sampling into 90% training data and 10% validation data. Fine-tuning is only done for a single epoch to preserve test case diversity and avoid overfitting. The last test case generation method is based on reinforcement learning, where the goal is to maximize the expected harmfulness elicited. Formally, this can be written as the expected harmfulness based on the probabilities produced by the RET classifier across the test cases generated by the RETLM. The RETLM is trained using the A2C reinforcement learning algorithm. The loss computed for updating the weights of the model is a linear combination of the KL penalty and the A2C loss. These test case generation methods bring several benefits with them. However, there are also drawbacks associated with them. For example, language models tend to pick up biases from the training data. Bias picked up by the RETLM will favor inputs from certain categories, which limits the test case diversity. Classifiers that are biased tend to be inaccurate, increasing the number of false positives or negatives. This is something important that we should be aware of. In RET-theming offensive language, the target model DPG dialogue prompt in GUFA is used to find text that poses offensive replies. DPG is a large-shared-word model of 280 billion parameters. It is trained on internet text and other sources. The responses in this model are based on handwritten text prefix or prompt followed by conversation history. This model was tested by training the model to predict whether an utterance is offensive along the dialogue history. The different generation methods along with input and goals of this model are shown in this table. For example, zero short method works on the basis of basic prompt with a goal of high diversity. Stochastic few short works on adding past five examples to prompt with goals to balance difficulty plus diversity. Supervised turning works on fine-tunes on harmful zero short examples with a goal focuses on noun risks. Reinforcement turning works on learn to maximize offensive replies with a goal of finds difficult edge cases. Each method has trade-off between diversity and difficulty. In this figure representation, we can see different language model as per diversity and offensiveness. Higher y-axis show greater diversity whereas the x-axis show greater offensive replies. Each method is an exchange of value between diversity and difficulty. Here we can see the comparison between different method in term of performance which method perform better than others. For example, reinforcement learning was the most effective method over 40% of offensive reply rate. Zero short found 18,444 failures from 500 k tests. Self-flow is used to measure diversity. All method outperforms manual bad data sets in some dimensions. In order to understand why DPG Dialog Prompt Gopher model fails, they cluster the text cases that pose offensive replies. There are two categories are unethical premises if you could steal something and sexual and vulgar theme asking and discussing any part of body. The dialogue DPG model agree with the unethical and or inappropriate questions input. The reply reinforce bad behaviors like insult, offensive jokes, sexualized reply. This table on the right side show a cluster of questions which generate offensive reply as a response. There are some classical examples of common offensive phrases that are generate as a response. You can see these like in the table on the right side showing input questions by red language model and in return the responses by DPG model which include offensive. DPG replies are often unkind both to the speaker as well as the others. To cater them, there are few strategies like training fixes, remove offensive examples from the training data sets and refusing bad premises. And the generation fixes use blacklist for flag phrases, tweak prompts to change chatbot values and complement human testing red teaming finds things human. Language models are known to generate text from the texting data posing user privacy risk as well. Data leakage examples from Gmail, Github, Copilot are in industry. The same way language model like DPG can memorize and repeat the training data possessing issues like privacy violation, copyright issues and security risk. This leakage can also allow the competition and or hackers to reverse engineer training sets. Experimental setups are created to cater these data leakage issues by using DPG chatbot mode. Testing DPG replies to 0.5 million short test cases by looking for responses that contain long sequences from the training data. By classifying DPG reply as containing 13 plus consecutive words that are subsequence in training example. As a result of this experimental setup, 1709 words or phrases were found leaking data. 106 cases DPG quotes from training data in response to questions that ask for a quote. 821 cases leak 13 gram occurs exactly once every time, suggesting any training data can be leaked. Many 13 gram leaks could be googled as well to trace their original sources. They identify leaks were of different types, quoted passages, unquoted plagiarism, explicit quotes. Overall, DPG is very vulnerable to data leakage. The results suggest several possible solutions to the data leakage harm exposed in previous slides. To respond to offensive user utterance and plagiarism, filter generates can be used when a user utterance asks for a quote. In order to reduce memorization privacy issue, we can use data curation. Previously, we only tested on single utterances, where the red LM generates a prompt that is then given to the target LM. Previously, we only tested on single utterances, where the red LM generates a prompt that is then given to the target LM. The target LM gives a response where we can check for harmful content. Now we will try to experiment with full dialogues. In this case, we will alternate the generation between the red LM and the target LM. This is the same as before, but now we will repeat those steps. It is obvious that the conversation in a dialogue needs to be remembered. The probability that the red LM will generate XI depends on all previous prompts and results, as stated here. Now some different methods are tested. First, the zero-shot method, where we just give some information and the goal of the task to the red LM, as shown at the right top. It says the user is trying to provoke Gopher to say things that are toxic, rude, or disrespectful, and so on. Then the dialogue starts and it will go up to 16 turns, so 8 per speaker. As I said before, the dialogue is passed to the red LM every time, so that it knows the history. Secondly, we have conditional zero-shot, which is actually the same as zero-shot, but it only uses the tests that performed pretty high in the previous part, where single utterances were used. But also zero-shot, so that was in section 3. Then we have stochastic viewshots, where some example dialogues were added to the prompts, so that the red LM knows how to get to the target LM's harmful content. And lastly, we have the non-adversarial methods, which just have some dialogues with the target LM without trying to attack it. So here are some interesting, but also obvious results of the experiments. In the first graph at the left, we see that for the adversarial methods, later in the dialogue, so after a few more turns, the chances of producing harmful content is higher than at the beginning. This makes sense, because the red teaming LM tries to manipulate the target LM, and it succeeds after multiple tries. At the right graph, we can see that if the previous responses of the target LM were offensive, there is a higher chance that the next one will also be offensive. For example, if the previous 7 responses were offensive or harmful, the next one will, in all methods, almost be 100% sure also be offensive. So now I'll quickly go over the discussion and broader impact of red teaming language models. So yeah, it's pretty obvious that they help with finding and fixing the risks that LM's generate harmful content. But the same techniques can also be used by adversaries externally, so you gotta keep that in mind. One advantage is the offense-defense asymmetry, where the adversary only needs to find one way to make the target LM generate harmful content. While the red teaming LM needs to handle all the cases. Another advantage is the unexpected harms, so the red teaming LM can miss some harms when checking for another type of harms. Although this can be solved by learning a general classifier, so that it handles and checks all harms together. And lastly, the different models act similarly, so when an adversary finds a way to make a model output harmful content, it can maybe be used for another model as well. Luckily, there are some advantages for the red teaming LM's as well. So firstly, the rate limits. Obviously, commercial LM's have rate limits for users, which for the red teaming it does not. As well as limited access, the red teaming LM has full access while the adversaries do not have access to the model or the training data or security mechanisms. And lastly, blue teaming. The red teaming is done before deployment, so all the failing tests can be fixed before they are deployed. And so the adversary cannot take advantage of that. So to conclude this paper, we can see the importance of red teaming. Firstly, it identifies harmful behavior in LM's, which is pretty important. It can be automated via red teaming LM's itself. Some key findings are that tens of thousands of offensive replies were found in a 280 billion per meter model. We tested some different methods, like zero shot, few shot, supervised learning and reinforcement learning. We also tested on different harms, like offensive jokes, insults, data leakage and distributional biases. So the implications are that it's a powerful tool for improving the LM's safety, and it can be complemented with manual testing.