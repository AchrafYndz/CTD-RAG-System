Hello, we're going to present the paper Distilling Step-by-Step. The main focus of this paper is to see how can we create models smaller in size and use less training data and computational power to be trained, but still outperform large language models that we're used to seeing. Okay, so we all know about the impressive capabilities that LLMs have shown. There are a few short abilities. It's a topic that we've discussed extensively in this course, but it is true that we haven't always given the importance to the drawbacks that these bigger models entail. Firstly, there's the most obvious one, the amount of data and computing power required associated with the training of the models. Apart from the need for specific hardware that supports this training, in this case, GPUs, which are not quite expensive, as you may know, I've written down that a 175 billion parameter model needs about 350 gigabytes of GPU memory. Just to put some context, the models that are currently being developed and rolled out, like, have three times as many parameters, which is just mental and puts into perspective how much this kind of work is necessary to keep models more constrained and smaller in size. There's also the problem with applicability, where there are some specific situations in which you cannot wait five, ten seconds for a response from chatGPT, for example, and you need kind of more immediate low latency responses. Think, for example, of medical or security reasons. Even though it's not on the paper, I wanted to briefly mention DeepSeq here, because I think it's really relevant to what we're going to be talking about. I don't know if you remember, but a month ago or something like that, there was this really big conversation going on with DeepSeq because they rolled out their new model. And the thing, the importance was not the performance that DeepSeq achieved, but more how they were able to train such a good model or comparable in performance to the ones that we're used to seeing without or with a lot of limitations imposed on both hardware and computing resources. And this just exemplifies how there's a big need and the literature is really looking into ways to obtain these kinds of smaller models that are more compact. So, OK, we want smaller models, but what kind of approaches can we take to achieve this model size? There's two current approaches to this problem. The first one is getting a smaller model and just fine tuning it for your specific application. It is true that this requires a lot of expensive human annotated datasets, which is not nice. That's why there's this method called destination, which I'm sure most of you know. Instead of using human annotators, what we do is kind of create a teacher-student architecture where the student is trying to mimic the performance of the bigger model, which is the teacher. This can be done in different ways by mimicking the output or mimicking the internal states of the neural network. This is really advantageous because it doesn't need human supervision. Instead, it's one model teaching it to the next one. The problem is, again, we need a lot of data in order to be able to use the distillation. So the main idea behind this project is how can we do distillation, but more effectively, I'm going to call it. And what did they propose is distilling step by step. So we've seen in this course how prompting change of thought reasoning can really help with the model's accuracy and performance. And the main idea behind distilling step by step is instead of treating the output of the models as if we were noisy labels, we are going to extract the reasoning behind those outputs and teach smaller models to reason in the same complex way as bigger models do. So the idea behind this kind of method is to end up with less training examples because we're just copying the reasoning and not the example per se. And also create smaller models that outperform bigger ones that we're used to seeing. OK, let's explain what is step by step distilling paradigm. Kind of a fancy name, but the idea is actually pretty simple. We start with a big powerful language model like GPT or something similar. And we use it to not only give us answers, but also to explain why it picked these answers. These explanations are called rationales. And they basically break down the model thinking in a step by step way. So there are two steps process. First, the big model generates labels and rationales. So for every example, it gives us the correct answers and explains how it got there. Second, we take that output and use it to train a smaller model. The smaller model learns from both the final answers and the explanations. Kind of like how you learn better when a teacher shows you the steps, not just the solutions. So yeah, it's a smart way to distill knowledge instead of just copying the answers. The smaller model learns how to reason through the problem like the big one does. Alright, now that we know why rationales are useful, let's talk about how we actually get them from large language models. The key technique used here is something called chain of thought prompting or CRT. What does that mean? Basically, instead of just asking the model for the final answer, we guide it to think out loudly, step by step, so we can see its reasoning process. So how do we do that? We give the model a prompt that includes few examples. Each example has three parts, the input or the questions, the second part is the rationales or the explanations, the third part is the final label which is the answer. This is what you can see in the example. At the top, there is an example where the model is shown how to answer a question and explain its reasoning. That's called few-shot CRT prompting because we are giving it a few examples to learn from. Then, once the model sees how it's done, we give it new unseen questions just like in the input section and ask it to do the same thing, explain its reasoning and give us the answers. And that's what's happening in the bottom of the example, in the output section. So in short, we are thinking the model to explain its thinking and then using that to generate new rationales and labels for data that we haven't labeled yet. And this is super useful when you want to train smaller models later or just understand what a large language model is thinking. Okay, now let's move on to explore how we typically train smaller models. There are basically two common approaches here. First, there is the fine-tuning approach, which is when we use labels created by humans to train a model. It's supervised learning and it usually gives great results. But also, it takes time and effort because someone has to label the data manually. Then, we have got a task distillation, and this is where the large language models really shine. Instead of relying on a human-annotated label, we use the predictions and rationales generated by the big model as training data for the smaller one. It's fast, more scalable, and super useful when we need it. don't have enough human label data. In both cases the goal is the same. We want a smaller model to make predictions that are as close as possible to the target label without too scum from human or a large language model. To do that we minimize something called label predictions loss which is usually calculated using gross entropy loss. That what is the formula as the slide showing. It just means we are averaging the loss over all the examples in the datasets to guide the model learning. So yeah, whether we are finding tuning the human label or restyling from large language model, the main idea is to help the smaller model to get better at predicting the right answer. Okay so here where things get even more interesting. So instead of just training the model to predict label only, the method shown here trains the model to do two things at once, predict the label and also generate the rationale behind it. This is what we call multitasking learning and in this case we are combining two related tasks into one training objective. Let's explain how this works. First we still have the label prediction loss the same as before. It measures how far off the model answers are from the correct one. But now we are also adding new loss term, the rationale generation loss. Let's check how close the model explanations are to the expected rationals. To balance the two we add a weight called lambda. That way we can control how much important we give to the rational part compared to the label predictions part. So the total loss function become a combo of both the loss for the labels and the loss for the rationals. This step helps the model not only give the right answers but also explain why, making it more transparent, trustworthy and even more useful for the downstream tasks. All right, now let's put everything we talked about into action and that's what exactly this example is about. Here we are looking at the full workflow of the styling step-by-step approach. We start on the left where we have different type of input data. We have some examples that include a sentence pair for natural language inference task, a multiple choice questions about golf and even simple math problem. All of this raw input is sent to the large language model. The job of the large language model here is to generate two things for each example. A rationale which is the explanations or the reasoning process and the label which is the final answer or the predictions. You can see the rationals written in blue in the middle column and the corresponding label on the right. Now here is the key part. We use this output to train a smaller model. So what do we feed the smaller model? We give it the same input example but now we include the label so it can learn to predict the right answer and also the rationale so it can also learn how to reason through it. The smaller model then learn to mimic the large language model not just in answers but in thinking too. So this whole pipeline is how we go from raw data to large language model generated explanations to training data for a smaller model which is more efficient and still think in step-by-step way. This is a pretty neat. What do you think? Okay let's wrap it up by talking about the benefit of this whole approach and how we actually make it work in practice. First of all one of the biggest win is that it's really improve interpretability and transparency because the smaller model learn to explain their answers we can better understand why they made a certain predictions and that make them way more trustworthy. It also help us accuracy since the model isn't just guessing it's learning how to reason properly. Second this method removes the need for the large language model at deployment that's a big deal because big model can be super expensive and slow to run. Once the smaller model is trained we can deploy it on its own without needing to call the large language model every time. Now in term of how it's actually implemented it's pretty clever as you saw in the last example in the previous slides. The smaller model learn to recognize and response to task specific prefix like label or rational so when you give it a prompt starting with label it know you that you want just the answers and if you use rationales as a prompt it give you an explanation so yeah it's a simple but powerful way to guide the model behavior and it's make deployment way easier. So next we will discuss the experimental setup. In this work they use the palm 540 billion parameters model as the large language model. It is used to both generate training data for the distilling step by step model and also as a baseline comparison. They also use a task specific downstream model of varying sizes and more specifically they use the so-called T5 model. The T5 model is trained to be the distilling step by step model but also the standard fine-tuned model which is used for comparisons. To evaluate the model performances they use four distinct data sets. Three of these are public NLP data sets so the first two data sets are the ESNLI and the ANLI and they assess the models abilities in natural language interference and specifically their effectiveness in determining logical relationship between two sentences. The third data set is the common sense question and answering data set and the final data set is the swamp data sets which consists of arithmetic math problems. In the first experiment they analyze how a reduction in training data affects the performance of the distilling step-by-step model. This experiment compares its performance to a traditional fine-tuned model which utilizes a hundred percent of the training data. The most significant difference in performance is observed when the model is tested on the ESNLI data set. Here the distilling step-by-step model only needs 12.5% of the full data set to outperform the traditional fine-tuned model that uses a hundred percent of the data. Additionally it outperforms the traditional model on the other three data sets as well although the reduction is not as substantial. On the ANLI data set it can reduce the data by 75%, on the CQA by 25% and on the swamp by 20%. This is also visualized in a graph that shows the model's performance when trained on human label data. The y-axis represent the test accuracy while the x-axis displays different sizes of the training set. The blue line shows the performance of the distilling step-by-step model across varying training set sizes and the orange line reflects the performance of the standard fine-tuning approach. The green line serves as a benchmark demonstrating the performance of the standard fine-tuned model when 100% of the training data is utilized. We can clearly see that the distilling step-by-step model outperforms the fine-tuned model across all sizes of the training set and this is also true for all the different data sets that it's tested on. This is another graph that illustrates the model performance when the training set size is varied. However in this case the training set consists of unlabeled data. data. Similar to this scenario with label data, we observe a significant improvement in the first two datasets, but also an overall improvement when using the distilling step-by-step model. The next aspect explored in this work was how a reduction in model size impacts the performance of the model. In this case, the distilling step-by-step model was compared to the standard fine-tuned model, and also as a baseline benchmark, they used the PAL model both under few-shot chain-of-thought prompting, and also when the PAL model was tuned using Pinto tuning. The main findings indicate that when comparing the distilling step-by-step model to the PAL model with 540 billion parameters, the distilling step-by-step model performs equally well at much smaller model sizes. On the ESNLI dataset, it only needed 220 million parameters to perform as well as the PAL model. On the ANLI and the SWAMP dataset, it required 770 million parameters, and on the CQA dataset, it needed 11 billion parameters. In this graph, we can see how different model sizes performed on the four datasets using labeled data. There is a notable difference in performance between the distilling step-by-step model and the original PAL model. However, the performance difference between the distilling step-by-step model and the standard fine-tuned model is not as significant. This is a similar graph, but in this case, they used unlabeled data. The distilling step-by-step model still outperforms the standard fine-tuned model at all different model sizes. And on the first three datasets, it can outperform the PAL model using a smaller model. However, this is not the case on the SWAMP dataset. To fix this issue, they further tuned both the distilling step-by-step model and the standard fine-tuned model on the ASDIV dataset, which consists of unlabeled data similar to that of the SWAMP dataset. When this was done, the distilling step-by-step model reached the same performance as the PAL model. Now we will combine the metrics of the previous experiments to find out what is the minimum model size and the least amount of training data needed to outperform a large-language model using our distillation step-by-step. This figure presents the results trained using the labelled dataset, where the x-axis represents the amount of training data required and the shades around the points indicate the model size. Examining the ESNLI results, we observed that step-by-step distillation significantly outperforms ViewShot, achieving superior performance with a model 2000 times smaller and using only 0.1% of the field's dataset. However, while ESNLI surpasses the LLM with 0.1% of the training data, ViewShot is consistently outperformed across all benchmarks using a significantly smaller model. Now comparing distilling step-by-step to standard fine-tuning, we find that fine-tuning often requires either more training data or a larger model to surpass. For example, ESNLI's fine-tuning score is significantly worse than LLM at 0.1% training data, whereas distillation achieves superior results, but when we increase the dataset, the results become on par with distillation step-by-step. For NLI, fine-tuning struggles to match the LLM's performance unless a larger model is used. In contrast, distillation surpasses the LLM's performance while requiring only 80% of the dataset. By looking at the results of the unlabeled dataset, we see the same pattern, where distilling step-by-step outperforms ViewShot with smaller models, and outperforms standard task distillation every time. As in previous experiments of the model size, here is the SVMP also performed worse, and as here the x-axis shows how much of the ASDIV dataset is added to the original. They also studied the impact of using different LLMs for generating rationales. To do this, they used the smaller 20 billion GPT NOX model instead of the 540 billion PAL model. They also questioned whether multitask learning approach, where they split up the rationales and the labels, is actually more effective than just concatenating them into single values. To do these experiments, they used the small task-specific models of 220 million parameters and 100% of the data on all datasets. If we look at the results, we can see that the 20 billion GPT NOX model performed overall better than the standard fine-tuning. However, this improvement is smaller than the larger 580 billion PAL model. This can be due to the fact that the larger PAL model provides higher quality rationales that are more beneficial for learning the task. For the multitask compared to the single task, we can see that the multitask approach, where the rationale and labels were split up, performed better than a single task, where the single task sometimes even performed worse by ANLI and the CQA than the standard fine-tuning. This highlights the fact that you need to treat rationales carefully, so they actually are beneficial, and validates the decision of the multitask training approach. Some limitations and ethical considerations in the paper. In order to use a few short chains of thought, it was required to produce a few example demonstrations, around 10 shots for all tasks. Because during training rationales need to be created, this leads to an increased training time, although very small, there is a small increased training time. This was not during tests, because the multitask design avoids this, because it is only needed to generate the labels without generating the rationales. And there is also research that offers evidence on a limited capability of more complex reasoning on any planning tasks, so this should be looked further into. Some ethical concerns are, because the smaller model learns from the bigger models, it is quite obvious that the smaller model will inherit the biases of the bigger model. To conclude this paper and our presentation, this scaling step-by-step allows us to reduce training data and model size by extracting rationales from large language models and creating smaller task-specific models. These small task-specific models even surpass the original large language model's performance. Because these models are significantly smaller, they also have the advantage that the hardware requirements are lower, and this allows for more efficient and cheaper usage. Some future direction this paper could be, instead of comparing to the few short chains of thought model that uses user-generated examples compared to a method where the rationales are elicited without any user examples. And in general, just more research about the rational quality effect, how can we improve it, for example, what's the impact of more complex reasoning and planning tasks.