and welcome to our explanation video about our paper. Our paper is named An Empirical Study of the Lama III Quantization from LLMs to MLLMs. So before we start, I will give a quick introduction to our paper to get a deeper understanding what the goal of our paper actually is. So I took a citation from the paper to understand this. So it says like, in this empirical study, we aim to analyze the capabilities of Lama III to handle the challenges associated with degradation due to quantization. So to understand what this means, we actually need to know what is quantization and what are those Lama III models. Okay, so what is quantization? Quantization is actually converting the weights of a model. So the weights of a model is usually 16 to 32 bits. And with quantization, we make them much smaller to eight, four, two, or even one bit. So we reduce actually the model size. And to do this, we lower the memory and the compute requirements. So instead of storing the full precise number, we stored approximate values using fewer bits. So we want to know, okay, if we do this quantization to reduce model size, what happens to the performance of a model? Does it perform as good as the precise numbers or does it perform bad? Does it have a drop in performance? So our second question was, what are the Lama models? We applied the quantization on. So the Lama models are actually a series of large language models designed by Meta. And these models vary from 7 billion parameters to 65 billion parameters. But in our study, we look specifically to the Lama III version. This is the most recent version. And this one is so good, it even achieved state-of-the-art performance on some tasks. So our goal is to try to compress the Lama III models with quantization due to resource limitations in some situations, and we want to see how their performance evaluate. So here we can see which models we are going to evaluate. We categorize them into two groups, the LLMs and the MLLMs. In the LLMs, we have the Lama III 8 billion version model and the Lama 70 billion version model. And both of them focuses on text tasks. And the other group, we have the MLLMs. These are the model versions. And we look at the Lama next 8 billion parameter model. And this one focuses on text and image tasks. Nine post-training quantization methods and two LoRa5 tuning methods are tested. These bandwidth range from one to eight bits. The study investigates how well these methods maintain Lama III performance under low bit conditions, addressing the challenge of performance degradation post-compression. For the PDQ methods, Lama III is tested on WikiText2, PTP, and a subset of C4 datasets using perplexity as a metric. Five zero-shot tasks further assess quantized performance using Common Sense QA expressed in accuracy. LoRaFT methods are assessed on the five-shot MMLU benchmark and the same five zero-shot datasets with consistent training data and hyperparameters to ensure fairness. The quantized Lama next 8 billion parameter model is evaluated on six visual language benchmarks to test multimodal capabilities. Now, let's dive into track one, post-training quantization. The big contribution here is evaluating Lama III 8 billion and 70 billion parameter model performance across nine PDQ methods from one to eight bits to benchmark its robustness and limits, which is a crucial key for deploying in low-resource settings. PDQ compresses models by matting floating-point weights into integers. Simply put, we use a general formula where weights are scaled by delta, shifted by a zero-point set, and clamped to an n-bit range. All the different models tweak this process differently to minimize errors. Let's dive into the results. Start with perplexity, or PPL, perplexity in language. Models measures a model's ability to predict the next word in a sequence. It quantifies the model's surprise when encountering new data, meaning lower surprise indicates the better prediction accuracy. For Lama III 8 billion parameter model, the 16-bit baseline shows PPL values of 6.1 on Wikitext 2, 9.2 on C4, and 10.6 on PTP. At four bits, methods like GPTQ, AWQ, SLIMLM, and QUIP maintain near-worthless performance as the PPL rises slightly for all the datasets. Even at three-bit precisions, the methods remain stable around the values observed in Lama III. However, when examining GPTQ and AWQ without grouping, we notice that their errors at least double compared to when grouping is applied. This happens because grouping helps optimize weights distribution, reducing quantization error and variance, whereas without grouping, the errors increases, leading to larger values. Further from two bits, we see that most of the methods collapses. Then, in the other part, if we first examine the RTN and smooth quant methods, we see that RTN produces results similar to the previously discussed methods, though slightly less effective. On the other hand, smooth quant performs poorly at four-bit precision. However, at six to eight bits, the results become comparable to those of Lama III at 16-bit precision. When examining the DBLLM and PBLM methods at two-bit precision, we see that they do not perform as well as other methods at four bits. However, they outperform those same methods when operating at two to three bits. As of 8.10, they exhibit significant instability, whereas DBLLM and PBLM maintain acceptable results with lower memory usage. Taking it a step further, PBLM begins to break down on the C4 and PTP datasets when using 1.7 bits. Meanwhile, PILM at just 1.1 bits still performs well on the Wikitext2 and PTP datasets, but also exhibits instability on the C4 dataset. For the Lama III 70 billion parameter model, the 16-bit baseline achieves PPL values of 2.9 on the Wikitext, 6.9 on C4, and 8.2 on PTP. SmoothQuant maintains identical performance to Lama III at six-bit and eight-bit precision. At four bits, SmoothQuant begins to collapse, while other methods still perform strongly. Even at three-bit precision, methods like GPTQ, AWQ, SLIMLM, and QUIP hold up well, but RTN starts to collapse. At two bits, most methods begin to degradate significantly, but GPTQ, AWQ, and QUIP remain more stable compared to their performance on the smaller Lama III eight billion parameter model. Their results even align with PBLM at two bits. Finally, PBLM and PILM manage to achieve reasonable performance even at ultra-low-bit width settings. For accuracy on a common-sense QA dataset, the SmoothQuant method using eight-bit quantization performance equally to Lama III eight billion parameter, while at six-bit, it shows a slightly drop in performance. At four-bit precision, most methods still match Lama III, but SmoothQuant starts to degradate significantly, performing at about a half the accuracy. This aligns with our earlier PPL observations, where SmoothQuant also collapsed at four-bit. Notably, SpinQuant, using eight-bit and four-bit activations, outperformed Lama III. At three-bit, RTN's performance declined sharply, while other methods experienced a slight drop as seen in the PPL results. At two-bit, performance deteriorates significantly across all methods. Finally, at ultra-low-bit width settings, accuracy is only about a half that of Lama III at 16-bit. For accuracy on a common-sense QA dataset, the SmoothQuant method using eight-bit quantization performed equally to the Lama III 70 billion parameter model, while at six-bit, it shows a slight drop in performance, but we can say it's almost the same. At four-bit precision, most methods still match Lama III, but SmoothQuant starts to degradate with almost 14% in accuracy. This also aligns with our earlier PPL observations, where SmoothQuant also collapsed at four-bit. At three-bit, RTN's performance declined sharply, while other methods experienced a slightly noticeable drop as also seen in the PPL results. At two-bit, performance deteriorates significantly across all methods. Finally, at ultra-low-bit width settings, accuracy is only about 60% of Lama III at 16-bit, but it's higher than when using the eight billion parameter model. Overall, for post-training quantization, four-bit methods show only a slight performance drop compared to the original 16-bit model, with no major differences between the various approaches. Moving to three-bit quantization, RTN suffers significantly, degradation over 10% loss on GPT-Q, AWQ, SLIM LLM, and QUIP maintains strong performance with less than 5% degradation compared to four-bit. Even at ultra-low-bit widths, two-bit and 1.1-bit, TBLLM and BILM achieve reasonable results, like we do the large batch fine-tuning and BILM's fine-grained silence partitioning. When quantizing both weights and activations, eight-bit models, both eight-billion parameter and 70-billion parameter models, retain near-mossless performance. As the bit width is further reduced, the eight-billion parameter model experience sharper performance drops, whereas the 70-billion parameter model degradates more readily, suggesting that larger models benefit from greater information redundancy. Okay, so now I will dive into the second track, namely Lora fine-tuning quantization. So the Lora fine-tuning quantization process exists of three steps. The first step is low-bit quantization of model weights, meaning the original pre-trained model weights are reduced to a lower bit precision, for example, four bits, to save memory and computation resources. This step applies the normal fluid quantization method. Second step is the addition of low-rank matrices. So low-rank matrices are introduced as learnable parameters and added to the pre-trained model's weights. This allows the model to adapt and improve its performance without altering the main and often resource-heavy core parameters. And lastly, we have the fine-tuning step. So these low-rank matrices are fine-tuned using the specific training data. This step ensures that the model adapts effectively to the task at hand while keeping the original weights intact. So this Lora fine-tuning technique is particularly advantageous because it minimizes memory requirement and computation costs, making it easier to deploy and fine-tune large models in constrained environments. Additionally, the process ensures that the core parameters remain unchanged, preserving the integrity of the original pre-trained model. So there are two Lora fine-tuning methods that are discussed and compared in the paper. The first method is QLORA. The QLORA method uses the four-bit normal fluid quantization method, which achieves significant memory reduction with minimal impact on model performance. And the second method is IRQLORA, which builds further on QLORA, but introduces two extra techniques. The first technique is information calibration quantization, meaning this method ensures that the quantized parameters retain the original information accurately, minimizing the loss of information during the compression process. And the second technique is information elastic connection. This approach allows Lora to utilize elastic representation transformations, enabling the model to adapt to diverse information more effectively. It enhances the fine-tuning process by maintaining flexibility in how information is represented. So now I will discuss the accuracy results of Lora fine-tuning on the LAMA3-8B model. So the base model is LAMA3. Secondly, we have the quantized model, which uses normal fluid quantization. And lastly, we have the two methods, which uses the QLORA and IRQLORA methods. These results are obtained on the ALPACA dataset, which is a general dataset. The first observation is that Lora fine-tuning quantization worsens performance below four bits. So fine-tuning LAMA3-8B using Lora fine-tuning on the ALPACA dataset does not compensate for quantization errors. Instead, it further degrades performance. The core reason for this is the LAMA3's exceptional baseline performance. So due to its large-scale pre-training, it limits the ability of fine-tuning on a smaller, low-quality dataset like ALPACA to compensate for errors introduced by quantization. Secondly, we can observe that IRQLORA performs better than QLORA. So IRQLORA outperforms QLORA due to its two extra techniques, namely information calibration quantization and information elastic connection. These features improve adaption even at low bit precision. So now I will discuss the accuracy results of Lora fine-tuning on the MMLU dataset. So here, two models are compared. The Lora fine-tuning on LAMA3-8B and the Lora fine-tuning on LAMA7B. So from these results, we can observe that LAMA3-8B still outperforms smaller models. So despite the limitations, 4-bit Lora fine-tuning LAMA3-8B still significantly outperforms smaller models such as LAMA7B. For example, QLORA 4-bit LAMA3-8B has an accuracy percentage of 56.7 and the QLORA 4-bit LAMA7B has an accuracy percentage of 38.4. So the key takeaway from these results is that a new LORA fine-tuning quantization strategy is needed for LAMA3 as traditional fine-tuning methods on small datasets do not effectively compensate for quantization loss in such high pre-trained models. So lastly, I will discuss the GPU memory usage and training time for LORA fine-tuning quantization. So QLORA and IRQLORA significantly enhance memory efficiency for LAMA models, reducing their memory footprint. However, they introduce inference bottlenecks due to the dequantization process leading to increased latency, as we can see in the table under the speed metric. This trade-off is often acceptable in memory-constrained environments. So further optimization techniques like hardware-specific tuning and algorithmic improvements could potentially mitigate these bottlenecks and enhance inference speed. We look at MLLM quantization, so multimodal models, specifically the LAVA Next 8 Billion model. So LAVA stands for Large Language and Vision Assistant. So this one focuses on text and image tasks. This one is built on top of the LAMA3 Next model, and we apply the post-training quantization technique on the LAMA3 part of this model. And we actually apply very low quantization, two-bit to four-bit quantization. And we look at the effects on both text and image-based tasks. So how did the quantization affect the model's ability to understand and describe images is what we want to see. And we look at the results, and we see that the four-bit quantization actually performs very well, almost as well as the original 16-bit model. So there is a very little performance drop in both text and visual language tasks. For the three-bit quantization model, there was a significant performance drop, 5% to 20%, but it still managed to generate somewhat coherent result, but with errors or less detail. And the two-bit quantization model actually performed very bad. It was a complete fail in multimodal tasks. And I want to show also another example how it performed. So an image of people working was shown with the text big companies at the top. And the question was to the model, okay, what does this image show? And we compared the different bits versions with each other. So the 16-bit version was the full precision version. It answered pretty good. And the four-bit version highlighted the workers and the environment, but didn't put them good together. The three-bit version recognized the workers and the construction, but there was a misinterpretation. And the two-bit version generated some repetitive sequences that didn't mean anything. Finally, I will discuss the conclusion of the paper. So despite some challenges, this research shows that AI is becoming more efficient without sacrificing much performance. This could lead to a future where powerful AI models run on everyday devices, making advanced capabilities like real-time translation and personalized assistance accessible to everyone. Instead of relying on massive data centers, AI could soon be in our pockets, transforming how we interact with technology.