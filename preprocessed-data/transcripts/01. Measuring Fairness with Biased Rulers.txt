Hello and welcome to our presentation about the paper Measuring Fairness with Biased Rulers, which is a comparative study on biased metrics for pre-trained language models. So what this presentation is about? In this presentation we'll explore the concept of fairness and bias in language models and the problems they cause to them. We will also discuss about the most commonly used fairness and bias metrics and see about their limitations in measuring bias in models. We will then see the correlation between these metrics and some measuring methods used. We will mention some challenges of the bias measures and then we will conclude with some recommendations about measuring fairness and bias. But first of all, what is bias and why it exists? Let's explain that with an example. We have Bert which is a language model that predicts he is a doctor 62% of the time but she is a doctor only 32% of the time. So this is called bias, this difference. Where is this problem of bias coming from though? The language models learn from raw and filtered data from the real world that already contain these biases and stereotypes. So the models learn to reproduce that and the bias affects applications where these models are used like search engines, generative AI tools like JGPT and even translation tools where they decide which gender to give to some words like the professions we saw. But where all this bias is coming from? There is a technique that is called transfer learning that is used for training models in AI tasks and it involves the pre-training part and the fine-tuning of the model. In the pre-training phase, the model learns features from large datasets of the real world and so it captures all the bias and the stereotypes that this data already has and this is called the intrinsic bias. In the fine-tuning phase, the model is adapted to a specific task using other datasets and so it captures additional bias from there and this is the extrinsic bias. Now we're going to see two types of embeddings. Embeddings are the representations of the words used by the language models. So we have first the non-contextualized embeddings like Word2Vec or GloVe where each word has its own vector no matter what the context of the word is in the sentence and the bias is measured using methods that are cosine similarity based like WIT or direct bias metric. On the other hand, we have the contextualized embeddings which are more recent like BERT or ELMO. In this case, each word may have more than one vector where each corresponds to the context of the word. Bias now is measured using sentence templates, token selection and layer selection. Because of this difference though, the metrics of these two embedding types cannot be compared. But why is measuring fairness so hard? First of all, metrics often do not agree with each other even when they are used in the same model. Secondly, there is no clear connection between the measuring of intrinsic and extrinsic bias and also the techniques that measure the bias in contextualized embeddings. They might use different seed words which are words that represent center categories, for example professions like doctor or nurse. They might use different templates which are sentence structures with seed words that predict the hidden word by the mask token here. One could say mask is a doctor and the other could say mask is a profession. Also, there are different similarity measuring methods like wheat, seed or PCA. So what is the goal of the paper study? First is to investigate some fairness metrics for contextualized models and compare them. Then is to analyze the correlation between different fairness metrics, templates and embedding methods. Then is to analyze the correlation between the intrinsic and extrinsic bias to find the most reliable fairness metrics. And finally, to find some challenges of the different languages except from English and measuring bias. Let's talk about now fairness measuring methods. Just to rewind from the previous slides, there are two measure ways to measure bias. Intrinsic bias which comes from free training and extrinsic bias from fine tuning. So let's start with the intrinsic bias measurement methods. First, I divided them into three graphs on how they work. And first one up there are the template-based methods. How they work? They just use templates and fill in the blank methods to spot stereotypes. Starting with discovery of correlations, the user's templates are shown in the example here. Likes to a masked object. Imagine it's like him. He likes to cook versus she likes to cook. The model fills in a blank and if the top predictions differ by gender or stereotype, so it's a bias flag. And the next one is lock probability bias score. It also uses this kind of templates and it checks the odds of he versus she but there is a one twist. It corrects for how often those words naturally appears in a training data. So it can be that the word he is used more common in a training data set. So that's why it leans towards he or she more depend on that. So it takes account into this thing also and it gives more fair results near the scores. Before going into the next graph, I wanted to mention about word embedding association test. It really hasn't been used in the paper for comparisons with other methods but it's the foundation of the couple of methods that they use it. So I thought it would be right to mention about it also. So what is it? It's designed for static embeddings like a word to break. Think of it like a way to measure how words associate in a vector space. It takes two sets of target words like a male names and female names and pairs them with the attributes like a career and family. So and using cuisine similarity it calculates if there is a bias like linking men to jobs and women to home life. This formula over here sums up those differences and yeah it worked well for a static models but it couldn't handle the overall the context. So that's why we will see now this adapted versions of a read. So yeah read extensions. The first one is sentence embedding association test. This model this method takes read for a and it's what they call a semantic with which templates like shown in this example. What does it mean? It's just they are super neutral, they are like a black canvas. So they don't imply anything specific about x like animations, no actions, or a context like that. X is a very happy engineer so there's nothing concrete about that. It uses paired CLS token to check association and it also tests stereotypes like an angry black woman which we were black women are unfairly seen as aggressive or something like that. And the next method is contextualized embedding association test. It just uses real read snippets and not abridged templates and target embeddings of a specific tokens not the CLS tokens and applies with association metrics. It's more realistic but context dependent because of the data set of the read. Now let's look at data set based methods. They are methods that uses data sets to spot a bias. The context association test is based on a stereo set data which consists of proficiency, gender, race, and religion but they mostly focused on gender ones. It checks if model picks stereotypes like a he for a programmer over she and scores how often that happens and if predictions are sensible. The next one is cross peers. It uses certain peers like he's a programmer versus she's a programmer and for each of them it scores perplexity and what is it? It's like a how surprised it is by the sentence so if there is a lower score for a he it means more expected and it can signal a bias. The all unmasked likelihood method is built on this but instead of just one prediction or a mask it looks at all words in a sentence and skips masks since real tasks don't use it. There is a table for these methods we have seen so far but there is also as you can see over here there is a PCA type methods. We have seen only association visit method type methods unlike the association methods PCA doesn't really doesn't directly measure links between words but it identifies structural patterns in an embedding space or how to say so we cannot obtain a numerical bias scores from these methods so they just didn't really consider it in the next section so they just put it there to show that these methods also exist. The next ones are extrinsic bias measurement methods and bias in real tasks bias and bios predicts someone's profession from their biography like let's see if there is a word called sentence like a software engineer with five years of experience so after fine-tuning should check how often it correctly identifies man versus woman as engineers a bigger gap means more gender bias and there is also vinyl bias it tests pronoun resolution bias like in this case the doctor helped the patient because he was kind so who is he and why is he so does it mean that language model always assume the doctor is a male or and the nurse is female so in that case it can be biased. And now let's move on to the methodology session of this work which experiments were carried out they carried out three experiments compatibility between templates compatibility between representations and so on and so forth. So let's start with templates compatibility between representations and compatibility between metrics and was focused on binary gender bias with respect to professions and for the first experiment compatibility between templates what they do they tested intrinsic bias measures that is bias measures applied during the actual task against intrinsic measures that is measures applied after pre-training task and evaluated these measures on five language models of bias detection and also use correlation analysis and relative entropy to compare template impacts and for the results for the first experiment what they have of course we can clearly observe that for templates most of these templates actually show weak correlation one another and the weak correlation is denoted by negative values and of course a few templates show positive correlation indicated by positive values with one another and on the table on the right hand side the last column gives us the values or the distance between each of these templates with t1. Distance here also means divergence or in other words distribution of biases within each of these templates so we see you can observe that the values are quite different indicating that each of these templates have their own biases so templates affect bias evaluation and now move to a second experiment which is compatibility between representations and what did they do here they simply tested different embedding selection methods that is serious embedding versus target world embedding by serious embedding we mean that we mean the embedding of the entire template is obtained from a serious token and by target world embedding we mean that the embedding of the entire template is obtained from the embedding of the target token and of course they conducted correlation analysis on bias scores using sentence embedding association tests and compare results across different templates and the results from a second experiment we observed that using a serious embedding approach the templates actually show weak correlation one another and in fact t3 and t6 which are semantically identical templates show weak correlation with each other on the contrary with target token embedding the templates show consistent correlation one another and on the rightmost side we have different embedding methods the pooled templates pool context first template will actually show strong correlation with one another so we see we can observe that embedding methods such as CLS embedding target token or embedding and other embedding methods actually affect bias evaluation and now in the third experiment what they do compatibility between metrics they tested the intrinsic bias metrics three of them on five language models that is bias in BIOS, window bias and skew and performed correlation analysis between intrinsic fairness measures and intrinsic bias metrics and what they obtained as results before I comment on the results we have to note that from SEAT till CalPERS these are intrinsic bias measures and bias in BIOS till skew are intrinsic bias measures we can observe that both bias in BIOS and skew give weak correlation with intrinsic bias measures of course the weak correlation is denoted by negative values meanwhile window bias gives strong correlation with intrinsic bias values and of course intrinsic bias measures sorry and of course we also observe that CalPERS actually does not match with intrinsic intrinsic bias measures we can observe CalPERS does not give bias values with intrinsic bias measure that's bias in BIOS window bias and skew so we can clearly see that different measures affect bias evaluation so we can conclude that in this work bias studies focus mainly on binary gender so future work can focus on more bias methods or gender methods like for example agenda where a person is considered as neutral does not have any gender and of course templates embeddings and metric choices impact bias evaluation leading to inconsistencies of course templates come from first experiment embeddings come from a second experiment and metric come from a third experiment and of course all these all these affect bias evaluation first second and third experiment and of course also note that fairness metrics are indicators not proof of fairness which which means that no no one single bias metric should actually give the fairness of a system or detect the fairness of a model of course they should not focus they should not be used as justification for unbiased models because these metrics actually do not align they actually give different results and so we cannot actually select which this metric is perfect or this metric gives this proof of fairness for this model and increasing and increasing measures do not always align also so that and fairness evaluation should extend beyond metrics to real world task. Thank you