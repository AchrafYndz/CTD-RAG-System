[Slide 1]
Neural Networks Toon Calders Dept. Computer Science University of Antwerp toon.calders@uantwerpen.be

[Slide 2]
Building Block: Perceptron â–ªA perceptron is a simplified model of a biological neuron. â–ªA neuron receives â€œinputâ€ from different other cells (other neurons or â€œsensorsâ€) â–ªIf total input exceeds a threshold, the neuron will â€œspikeâ€ (i.e., emit a signal) â€¢ Human brain consists out of approximately 86 billion neurons Picture from Wikipedia (http://en.wikipedia.org/wiki/Chemical_synapse)

[Slide 3]
Perceptron 3 Î£ ïƒ² W1 W2 W3 X1 X2 X3 Output Input Weights Output â–ªA perceptron takes inputs and computes a function â–ªOutput depends on weights and an activation function b

[Slide 4]
Perceptron 4 Î£ ïƒ² W1 W2 W3 X1 X2 X3 Output Input Weights Output â–ªA perceptron takes inputs and computes a function â–ªOutput depends on weights and an activation function b

[Slide 5]
Perceptron 5 â–ªA perceptron takes inputs and computes a function â–ªOutput depends on weights and an activation function â–ªExample of an activation function: â–ªThe perceptron on last page computes the function:

[Slide 6]
Perceptron 6 â–ªA perceptron takes inputs and computes a function â–ªOutput depends on weights and an activation function â–ªExample of an activation function: â–ªThe perceptron on last page computes the function:

[Slide 7]
Examples - Perceptrons 7 Î£ ïƒ² 3 1 X Y -5

[Slide 8]
Examples - Perceptrons 8 Î£ ïƒ² -1 1 X Y 0

[Slide 9]
Examples - Perceptrons 9 â–ªEach choice of W1, W2, B corresponds to another function, mapping (x,y) to a number

[Slide 10]
Perceptron Learning - Algorithm â–ªGiven a dataset, learn the parameters of a perceptron that produces similar output x1 x2 y 1 3 0 1 4 0 2 4 0 4 2 1 5 1 1

[Slide 11]
Perceptron Learning - Algorithm â–ªGiven a dataset, learn the parameters of a perceptron that produces similar output â–ªPerceptron learning = learning weights = training the network â–ªIteratively refine the model until it fits the examples â–ªTraining proceeds as follows: â–ªStart with random weights â–ªRepeat until â€œgood enoughâ€: For each training example (x,y,label): slightly change the weights to improve prediction example

[Slide 12]
Extending to Multiple Perceptrons â–ªOne perceptron has limited representational power â–ªWe can combine multiple perceptrons to create a more complex neural network that can express more complex functions x1 xn â€¦ y1 x2 y2 ym y Hidden layer

[Slide 13]
Example : Multi-Layer Perceptron Network x1 x2 y1 1 -1 1 0 0 1 y2 -2 1 -1.5 y 1 1

[Slide 14]
Training a Neural Network 14 â–ªExactly as for one perceptron: â–ªNeural Network learning = learning weights = training the network â–ªIteratively refine the model until it fits the examples â–ªTraining proceeds as follows: â–ªStart with random weights â–ªRepeat until â€œgood enoughâ€: For each training example (x,y,label): slightly change the weights to improve prediction â–ªTraining a NN can be time-consuming example 1 One complete run through the dataset is called an â€œepochâ€

[Slide 15]
Deep Learning 15 â–ªDeep learning = learning of large NNs â–ªNNs were already studied in the 60s â–ªWhatâ€™s new?

[Slide 16]
What is so great about Neural Networks ? â–ªLittle to no model bias â–ªThey can present any function (if sufficiently large) â–ªIf there exists a mapping from features to labels, a neural network can in principle learn it â–ªBuilt-in feature engineering â–ªNo need for inventing complex features to capture meaningful patterns in the data â–ªBut, comes at a cost! â–ªLot and lots of data required! â–ªHuge computational demands â–ªExpensive hardware

[Slide 17]
Overview 17 â€¢ â€œTraditionalâ€ NLP â€¢ Word embeddings â€¢ Transformers

[Slide 18]
Textual data 18 https://www.imdb.com/title/tt0210075/reviews?ref_=tt_urv X1 X2 X3 X4 â€¦ label 0.3 0.5 1.4 -0.2 â€¦ POS 1.4 0.2 1.7 2.4 â€¦ NEG â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ M X1 X2 X3 X4 â€¦ label 0.3 0.5 1.4 -0.2 â€¦ ???

[Slide 19]
Representation for Text 19 â–ªPreprocessing: Girlfight follows a project dwelling New York high school girl from a sense of futility â€¦ girlfight follows a project dwelling newyork high school girl from a sense of â€¦ Punctuation removal, Lower casing, Entity recognition Stopword removal, Tokenization, Counting girlfight 1 follows 3 project 2 dwelling 1 newyork 2 high 2 school 3 girl 7 sense 2 â€¦

[Slide 20]
Representation for Text 20 â–ªâ€œBag-of-wordsâ€ dwelling newyork way high good â€¦ label 1 2 0 2 0 â€¦ neg 0 3 2 7 2 â€¦ pos â€¦ neg â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ â€¦ girlfight 1 follows 3 project 2 dwelling 1 newyork 2 high 2 school 3 girl 7 sense 2 â€¦

[Slide 21]
Overview 21 â€¢ â€œTraditionalâ€ NLP â€¢ Word embeddings â€¢ Transformers

[Slide 22]
Neural Nets for Natural Language Processing 22 â–ªWord embedding = representation of words/texts as a vector of numbers â–ªBanana â†’ (0.3, 5.8, 7.3, 0.1) â–ªFather â†’ (0.4, 0.7, 1.2, 0.4) â–ªBaby â†’ (0.3, 0.6, 1.5, 3.0) â–ªâ€¦ â–ªWhy? Hundreds of algorithms work with numbers. Word2Vec is like an â€œadaptorâ€

[Slide 23]
Embeddings are Not Random 23 â–ªWords used in similar contexts should have similar vectors delicious aweful good bad intelligent obnoxious tasty dense rotten PEOPLE POSITIVE NEGATIVE excellent trustworthy FOOD

[Slide 24]
How are Embeddings Learned from Data ? â–ªWe turn the problem into a game Guess the word ! Florida man charged for assault with a deadly weapon after throwing alligator into Wendyâ€™s drive thru ?

[Slide 25]
How are Embeddings Learned from Data ? â–ªAI will learn a model to predict the word â–ªEasy to generate test data: Florida man charged for assault charged man charged for assault with for charged for assault with a assault for assault with a deadly with ? ? ? ?

[Slide 26]
How are Embeddings Learned from Data ? â–ªAI will learn a model to predict the word â–ªThe structure of the model is fixed: â–ªBecomes an optimization problem Florida (0.5, 0.3, 0.7, 0.9) Man (0.2, 0.1, 0.6, 0.8) For (0.5, 0.4, 0.1, 0.9) assault (0.1, 0.9, 0.3, 0.2) Model 1 Model 1 Model 1 Model 1 Model 2 sought blamed for a an convicted charged the man men â€¦ Florida P 0.1 0.1 0 0 0 0.1 0.7 0 0 0 0 0

[Slide 27]
How are Embeddings Learned from Data ? â–ªAI will learn a model to predict the word â–ªEasy to generate test data â–ªThe structure of the model is fixed â–ªWe are only interested in a part of the model: This part of the model captures semantic information in a vector that allows to play the â€œguessing gameâ€ Florida (0.5, 0.3, 0.7, 0.9) Model 1

[Slide 28]
Word2vec Captures Semantic Information 28 ğ‘šğ‘ğ‘›âˆ’ğ‘¤ğ‘œğ‘šğ‘ğ‘› â‰ˆğ‘˜ğ‘–ğ‘›ğ‘”âˆ’ğ‘ğ‘¢ğ‘’ğ‘’ğ‘› ğ‘ ğ‘™ğ‘œğ‘¤ğ‘’ğ‘Ÿâˆ’ğ‘ ğ‘™ğ‘œğ‘¤ â‰ˆğ‘ â„ğ‘œğ‘Ÿğ‘¡ğ‘’ğ‘Ÿâˆ’ğ‘ â„ğ‘œğ‘Ÿğ‘¡

[Slide 29]
Amazing Applications: Sentiment Analysis 29 https://www.micc.unifi.it/projects/advanced-web-applications/sentiment-analysis-of-tweets-from-twitter/

[Slide 30]
Demo word embeddings 30 â–ªhttps://projector.tensorflow.org/

[Slide 31]
Example: CV Screening ? â–ªFirst transforms data into numbers via WordToVec: â–ªThen apply any of the classification methods we have seen This Photo by Unknown Author is licensed under CC BY-NC-ND transform C1 C2 C3 C4 â€¦ Hire? 0.5 7.3 0.34 3.1 â€¦ Yes

[Slide 32]
Overview 32 â€¢ â€œTraditionalâ€ NLP â€¢ Word embeddings â€¢ Transformers

[Slide 33]
Transformers â–ªUntil the advent of transformers, LSTM-based RNNs were state-of-the- art for sequence-to-sequence problems â–ªProblem with the RNN architecture: â–ªHard to take long-term dependencies into account â–ªThe transformer architecture makes the path between the information needed to understand/translate a token shorter

[Slide 34]
Transformer: High level view Alammar, J (2018). The Illustrated Transformer [Blog post]. Retrieved from https://jalammar.github.io/illustrated- transformer/

[Slide 35]
Attention Mechanism â–ªThe attention mechanism allows the network to concentrate on specific tokens to enrich the encoding of a token â–ªExample: The animal did not cross the road because it was too tired. â–ªWho or what is too tired? â–ªExample: The animal did not cross the road because it was too wide. â–ªWho or what is too wide?

[Slide 36]
Decoder â–ªThe decoder produces the next word based on input and preceding words â–ªA similar architecture is used encoder decoder I am a student <start> je

[Slide 37]
Decoder â–ªThe decoder produces the next word based on input and preceding words â–ªA similar architecture is used encoder decoder I am a student <start> je suis

[Slide 38]
Decoder â–ªThe decoder produces the next word based on input and preceding words â–ªA similar architecture is used encoder decoder I am a student <start> je suis une

[Slide 39]
Decoder â–ªThe decoder produces the next word based on input and preceding words â–ªA similar architecture is used encoder decoder I am a student <start> je suis Ã©tudiante une

[Slide 40]
Decoder â–ªThe decoder produces the next word based on input and preceding words â–ªA similar architecture is used encoder decoder I am a student <start> je suis Ã©tudiante une <stop>

[Slide 41]
Training a transformer â–ªWe need a huge corpus of input-output pairs I am a student â€“ Je suis une Ã©tudiante I am Manuel from Barcelona - Je suis Manuel de Barcelone I learn English from a book - J'apprends l'anglais Ã  partir d'un livre â€¦ â–ªFor a chatbot: learn to predict the next word/sentence in a conversation

[Slide 42]
BERT â–ªBERT stands for Bidirectional Encoder Representations from Transformers â–ªTrained in a very peculiar way such that the embedding vectors contain a lot of information â–ªmissing words â–ªcontext of a word â–ªconsistency between sentences Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).

[Slide 43]
https://youtu.be/ioGry-89gqE

[Slide 44]
OpenAI GPT Model â–ªThe OpenAI GPT â–ªGeneral-purpose â–ªPre-trained â–ªTransformer â–ªTrained with next-word prediction â–ªonly attend to previous tokens â–ªpredict next word based on final embedding Easy to obtain training data; no need for bilingual corpus Embeddings themselves are of interest; e.g., build spam prediction on top Devlin, Jacob, et al. "Bert: Pre-training of deep bidirectional transformers for language understanding." arXiv preprint arXiv:1810.04805 (2018).

[Slide 45]
OpenAI GPT-2â€™s Famous Example https://openai.com/blog/better-language-mode

[Slide 46]
46 Building an application based on LLMs https://medium.com/artefact-engineering-and-data-science/how-to-train-a- language-model-from-scratch-without-any-linguistic-knowledge-11acaa933e84

[Slide 47]
Zero-Shot Learning 47 â–ªLLMs are trained on huge amounts of data â€“ surprising powerful

[Slide 48]
Example Zero-Shot Learning 48 Instead of learning a model for a task, design a prompt: â€œI bought a bike from you 3 years ago; could you retrieve the make?â€ Annotate the request; add context: â€œTake the following context into account: User purchased: [List of user transactions] Product catalogue: [List of products] Answer the following question by mr X: I bought a bike from you 3 years ago; could you retrieve the make?â€ Send anotated request to a general-purpose LLM

[Slide 49]
Conclusion â–ªNatural Language Processing dominated by Deep Neural Network Models â€¢ Word embeddings â†’ classification â€¢ Transformers â†’ Sequence to sequence tasks â€¢ Translation, chat-bots, summarization, â€¦ â–ªSimilarly as for image recognition networks can pre-trained models be downloaded, often for free â€¢ New standard : offered as a service ? â–ªSome issues to take into account: â–ªFine-tuning on company data may cause privacy issues â–ªLLMs can show unpredictable behavior, biases, stereotypes
